# Data Science Competitions Collection - Machine Learning Kaggle Challenges

Este directorio contiene una colecci√≥n completa de soluciones para competiciones de Kaggle, implementando t√©cnicas avanzadas de machine learning, an√°lisis exploratorio de datos y ingenier√≠a de caracter√≠sticas. Cada proyecto representa una soluci√≥n end-to-end desde la exploraci√≥n inicial hasta la generaci√≥n de submissions competitivas.

## üéØ Descripci√≥n General

La colecci√≥n demuestra competencia en m√∫ltiples √°reas de machine learning: **procesamiento de lenguaje natural (NLP)**, **series temporales**, **clasificaci√≥n binaria y multiclase**, **regresi√≥n**, y **ensemble learning**. Los proyectos est√°n organizados por tipo de problema y complejidad t√©cnica, proporcionando soluciones optimizadas para cada dominio espec√≠fico.

## üìä Tecnolog√≠as y Frameworks Utilizados

| Categor√≠a | Tecnolog√≠as | Competiciones |
|-----------|-------------|-----------|
| **Lenguajes** | Python 3.x | Todas las competiciones |
| **Machine Learning** | Scikit-learn, CatBoost, XGBoost, LightGBM | Titanic, House Prices, Store Sales, Space-Titanic |
| **Deep Learning** | PyTorch, Transformers (Hugging Face) | EEDI NLP, Disaster Tweets |
| **NLP Models** | DistilBERT, Flan-T5 | EEDI, Disaster Classification |
| **Ensemble Methods** | StackingClassifier, RandomForest, Gradient Boosting | Titanic, House Prices, BrisT1D |
| **Time Series** | Temporal Feature Engineering, CatBoost Regressor | Store Sales, BrisT1D |
| **An√°lisis de Datos** | Pandas, NumPy | Todas las competiciones |
| **Visualizaci√≥n** | Matplotlib, Seaborn | EDA en todas las competiciones |
| **Deployment** | Kaggle Notebooks, Jupyter | Todas las competiciones |

## üèóÔ∏è Estructura de Competiciones

```
competitions/
‚îÇ
‚îú‚îÄ‚îÄ Blood/                          # BrisT1D Blood Glucose Prediction
‚îÇ   ‚îú‚îÄ‚îÄ Data/                       # Datasets de entrenamiento y prueba
‚îÇ   ‚îî‚îÄ‚îÄ Notebook/                   # Jupyter notebooks con an√°lisis
‚îÇ
‚îú‚îÄ‚îÄ House_Prices/                   # House Prices Advanced Regression
‚îÇ   ‚îú‚îÄ‚îÄ Data/                       # Datasets de la competici√≥n
‚îÇ   ‚îî‚îÄ‚îÄ Notebook/                   # Implementaci√≥n y an√°lisis
‚îÇ
‚îú‚îÄ‚îÄ Misconceptions/                 # EEDI Mining Misconceptions in Mathematics
‚îÇ   ‚îú‚îÄ‚îÄ Data/                       # Datasets educativos
‚îÇ   ‚îî‚îÄ‚îÄ Notebook/                   # Fine-tuning NLP
‚îÇ
‚îú‚îÄ‚îÄ Natural_Disaster/               # Natural Language Processing with Disaster Tweets
‚îÇ   ‚îú‚îÄ‚îÄ Data/                       # Datasets de clasificaci√≥n de texto
‚îÇ   ‚îî‚îÄ‚îÄ Notebook/                   # DistilBERT implementation
‚îÇ
‚îú‚îÄ‚îÄ Sales_Time_Forecasting/         # Store Sales Time Series Forecasting
‚îÇ   ‚îú‚îÄ‚îÄ Data/                       # M√∫ltiples fuentes de datos temporales
‚îÇ   ‚îî‚îÄ‚îÄ Notebook/                   # An√°lisis de series temporales
‚îÇ
‚îú‚îÄ‚îÄ Space-Titanic/                  # Spaceship Titanic Sci-Fi Challenge
‚îÇ   ‚îú‚îÄ‚îÄ Data/                       # Datasets sci-fi
‚îÇ   ‚îî‚îÄ‚îÄ Notebook/                   # Clasificaci√≥n avanzada
‚îÇ
‚îî‚îÄ‚îÄ Titanic/                        # Classic Titanic Survival Prediction
    ‚îú‚îÄ‚îÄ Data/                       # Datasets cl√°sicos
    ‚îî‚îÄ‚îÄ Notebook/                   # Ensemble methods
```

## üèÜ Competiciones y Resultados

### 1. **BrisT1D Blood Glucose Prediction** ü©∫
- **Tipo:** Regresi√≥n - Predicci√≥n de Series Temporales M√©dicas
- **Modelo Principal:** XGBoost con RandomizedSearchCV
- **T√©cnicas:** Transformaci√≥n Logar√≠tmica, Ingenier√≠a Temporal, Optimizaci√≥n de Hiperpar√°metros
- **M√©trica:** RMSE en escala original
- **Innovaci√≥n:** Pipeline m√©dico-espec√≠fico para datos gluc√©micos

### 2. **EEDI Mining Misconceptions in Mathematics** üß†
- **Tipo:** NLP - Fine-Tuning de Transformers
- **Modelo Principal:** Flan-T5 Fine-Tuned
- **T√©cnicas:** Transfer Learning, Prompt Engineering, Embeddings Sem√°nticos
- **M√©trica:** mAP@25 (Mean Average Precision)
- **Innovaci√≥n:** Identificaci√≥n autom√°tica de conceptos err√≥neos educativos

### 3. **Store Sales Time Series Forecasting** üìà
- **Tipo:** Regresi√≥n - Forecasting Multivariado
- **Modelo Principal:** CatBoost Regressor
- **T√©cnicas:** Fusi√≥n de M√∫ltiples Fuentes, Feature Engineering Temporal, Early Stopping
- **M√©trica:** RMSE
- **Innovaci√≥n:** Integraci√≥n de factores econ√≥micos y eventos especiales

### 4. **Natural Language Processing with Disaster Tweets** üö®
- **Tipo:** Clasificaci√≥n Binaria - NLP
- **Modelo Principal:** DistilBERT Fine-Tuned
- **T√©cnicas:** Layer Freezing, Preprocesamiento de Redes Sociales
- **M√©trica:** F1 Score
- **Innovaci√≥n:** Clasificaci√≥n eficiente con fine-tuning selectivo

### 5. **House Prices Advanced Regression** üè†
- **Tipo:** Regresi√≥n - Predicci√≥n de Precios
- **Modelo Principal:** CatBoost + Stacking
- **T√©cnicas:** An√°lisis de Correlaci√≥n, Imputaci√≥n KNN, Meta-Modelado
- **M√©trica:** RMSE
- **Innovaci√≥n:** Pipeline de limpieza automatizado y ensemble robusto

### 6. **Spaceship Titanic** üöÄ
- **Tipo:** Clasificaci√≥n Binaria - Sci-Fi Context
- **Modelo Principal:** CatBoost Classifier
- **T√©cnicas:** Feature Engineering Contextual, Manejo de Categ√≥ricas
- **M√©trica:** Accuracy
- **Innovaci√≥n:** Ingenier√≠a de caracter√≠sticas espaciales tem√°ticas

### 7. **Classic Titanic Survival Prediction** ‚öì
- **Tipo:** Clasificaci√≥n Binaria - Problema Cl√°sico
- **Modelo Principal:** StackingClassifier (4 algoritmos + Ridge)
- **T√©cnicas:** Ensemble Heterog√©neo, Feature Engineering Avanzada
- **M√©trica:** Accuracy
- **Innovaci√≥n:** Combinaci√≥n √≥ptima de m√∫ltiples paradigmas de ML

## üìà M√©tricas y Performance

### Performance de Modelos por Competici√≥n

| Competici√≥n | Modelo Principal | M√©trica | T√©cnica Destacada |
|----------|------------------|---------|-------------------|
| **Titanic** | StackingClassifier | Accuracy | Ensemble de 4 algoritmos |
| **House Prices** | CatBoost + Stacking | RMSE | Manejo autom√°tico de categ√≥ricas |
| **Store Sales** | CatBoost Regressor | RMSE | Transformaci√≥n log + Early Stopping |
| **BrisT1D** | XGBoost Optimizado | RMSE | RandomizedSearchCV |
| **EEDI NLP** | Flan-T5 Fine-Tuned | mAP@25 | Meta-learning especializado |
| **Disaster Tweets** | DistilBERT | F1 Score | Transfer learning eficiente |
| **Space-Titanic** | CatBoost Classifier | Accuracy | Feature engineering contextual |

### T√©cnicas Avanzadas Implementadas

#### **Ensemble Learning**
- **StackingClassifier:** Titanic, House Prices
- **Gradient Boosting:** CatBoost, XGBoost, LightGBM en m√∫ltiples proyectos
- **Meta-Learning:** Ridge regression como meta-estimador

#### **NLP y Transformers**
- **Fine-Tuning:** DistilBERT, Flan-T5
- **Prompt Engineering:** Prompts estructurados para contexto educativo
- **Transfer Learning:** Aprovechamiento de modelos pre-entrenados

#### **Feature Engineering**
- **Temporal:** Extracci√≥n de patrones estacionales y c√≠clicos
- **Domain-Specific:** Caracter√≠sticas contextuales por tipo de problema
- **Transformaciones:** Logar√≠tmicas, binning, categorizaci√≥n

#### **Optimizaci√≥n**
- **Hyperparameter Tuning:** RandomizedSearchCV, Grid Search
- **Early Stopping:** Prevenci√≥n de overfitting autom√°tica
- **Cross-Validation:** Estrategias apropiadas por tipo de problema

## üî¨ Patrones de √âxito Identificados

### **Preprocessing Consistente**
```python
# Patr√≥n est√°ndar aplicado en todas las competiciones
def preprocess_pipeline(train, test):
    # 1. Manejo de nulos diferenciado por tipo
    for column in train.columns:
        if train[column].dtype == 'object':
            train[column].fillna(train[column].mode()[0], inplace=True)
        else:
            train[column].fillna(train[column].mean(), inplace=True)
    
    # 2. Feature engineering domain-specific
    # 3. Encoding consistente train/test
    # 4. Transformaciones de distribuci√≥n
    return train_processed, test_processed
```

### **Ensemble Architecture**
```python
# Arquitectura probada en m√∫ltiples competiciones
StackingClassifier(
    estimators=[
        ('rf', RandomForestClassifier(random_state=42)),
        ('cb', CatBoostClassifier(silent=True)), 
        ('lgb', LGBMClassifier()),
        ('xgb', XGBClassifier(eval_metric='logloss'))
    ],
    final_estimator=RidgeClassifier(),
    cv=5
)
```

### **Evaluation Framework**
```python
# Marco de evaluaci√≥n implementado consistentemente
def evaluate_model(model, X_val, y_val, problem_type):
    if problem_type == 'classification':
        # Accuracy, F1, Confusion Matrix
        predictions = model.predict(X_val)
        accuracy = accuracy_score(y_val, predictions)
        
    elif problem_type == 'regression':
        # RMSE en escala original para interpretabilidad
        predictions = model.predict(X_val)
        if log_transformed:
            predictions = np.expm1(predictions)
            y_val = np.expm1(y_val)
        rmse = np.sqrt(mean_squared_error(y_val, predictions))
    
    return metrics_dict
```

### **Feature Engineering Patterns**
```python
# Patrones exitosos por tipo de datos

# 1. Variables compuestas
df['Family_Size'] = df['SibSp'] + df['Parch'] + 1
df['Total_Spend'] = df[spending_columns].sum(axis=1)

# 2. Extracci√≥n de informaci√≥n estructurada
df['Title'] = df['Name'].str.extract(r', (.*?)\.')
df[['Deck', 'Num', 'Side']] = df['Cabin'].str.split('/', expand=True)

# 3. Categorizaci√≥n inteligente
def categorize_age(age):
    if age <= 5: return 'Child'
    elif age <= 18: return 'Teenager'
    elif age <= 60: return 'Adult'
    else: return 'Senior'

# 4. Transformaciones de distribuci√≥n
df['log_target'] = np.log1p(df['target'])  # Para targets con sesgo
```

## üìä Estad√≠sticas de la Colecci√≥n

### **M√©tricas Generales**
- **üèÜ 7 Competiciones Completadas:** Desde clasificaci√≥n b√°sica hasta NLP avanzado
- **üìà 12+ Algoritmos Implementados:** Desde regresi√≥n lineal hasta transformers
- **üéØ 4 Tipos de Problemas:** Clasificaci√≥n, regresi√≥n, NLP, series temporales
- **üíæ 50+ GB de Datos Procesados:** Diversos formatos y dominios
- **‚è±Ô∏è 150+ Horas de Modelado:** Desde EDA hasta submissions

### **Distribuci√≥n por T√©cnicas**
- **Machine Learning Cl√°sico:** 45% (Ensemble, √°rboles, regresi√≥n)
- **Deep Learning/NLP:** 35% (Transformers, fine-tuning)
- **Series Temporales:** 15% (Forecasting, feature engineering temporal)
- **Optimizaci√≥n Avanzada:** 5% (Hyperparameter tuning, cross-validation)

### **Complejidad T√©cnica**
- **Nivel Principiante:** Titanic (clasificaci√≥n b√°sica con ensemble)
- **Nivel Intermedio:** House Prices, Space-Titanic (feature engineering avanzado)
- **Nivel Avanzado:** Store Sales, BrisT1D (series temporales, optimizaci√≥n)
- **Nivel Experto:** EEDI, Disaster Tweets (NLP, fine-tuning transformers)

## üöÄ Entornos Recomendados

### **Kaggle Notebooks (Recomendado)**
- **Ventajas:** GPU gratuita, datasets integrados, submission directa
- **Uso:** Competiciones oficiales, desarrollo r√°pido
- **Configuraci√≥n:** T4 x2 GPU para proyectos de NLP

### **Google Colab**
- **Ventajas:** GPU gratuita, f√°cil sharing, instalaci√≥n flexible
- **Uso:** Experimentaci√≥n, desarrollo iterativo
- **Configuraci√≥n:** Runtime GPU para modelos pesados

### **Jupyter Local**
- **Ventajas:** Control total, debugging avanzado, recursos locales
- **Uso:** An√°lisis exploratorio detallado, desarrollo de pipelines
- **Configuraci√≥n:** Entorno conda/venv con requirements espec√≠ficos

## üîß Reproducibilidad

### **Control de Aleatoriedad**
```python
# Configuraci√≥n est√°ndar en todos los proyectos
import random
import numpy as np

RANDOM_STATE = 42
random.seed(RANDOM_STATE)
np.random.seed(RANDOM_STATE)

# En modelos espec√≠ficos
model = RandomForestClassifier(random_state=RANDOM_STATE)
X_train, X_test = train_test_split(..., random_state=RANDOM_STATE)
```

### **Gesti√≥n de Dependencias**
- **Requirements espec√≠ficos:** Cada competici√≥n incluye sus dependencias exactas
- **Versiones fijas:** Para garantizar reproducibilidad de resultados
- **Documentaci√≥n de setup:** Instrucciones paso a paso en cada README

### **Estructura Consistente**
- **Notebooks organizados:** Secciones claras y documentadas
- **Datos preservados:** Datasets originales + procesados
- **Submissions guardadas:** Archivos de env√≠o para referencia

## üìû Contacto y Colaboraci√≥n

Para consultas t√©cnicas sobre implementaciones espec√≠ficas, discusiones sobre estrategias de competici√≥n, o colaboraciones en futuros challenges de Kaggle, no dudes en contactar.

## üìó Referencias y Recursos

- **Kaggle Competitions:** Plataforma oficial para todas las competiciones
- **Scikit-learn Documentation:** Framework principal para ML cl√°sico
- **Hugging Face Transformers:** Modelos y t√©cnicas de NLP modernas
- **CatBoost Documentation:** Gu√≠as y best practices para gradient boosting

---

*Esta colecci√≥n representa una demostraci√≥n comprehensiva de t√©cnicas modernas de machine learning aplicadas a competiciones reales de Kaggle, desde problemas cl√°sicos de clasificaci√≥n hasta challenges cutting-edge de NLP, proporcionando soluciones robustas y competitivas para cada dominio espec√≠fico.*