# Clasificaci√≥n de Desastres Naturales con DistilBERT - NLP Fine-Tuning Project

Este proyecto implementa una soluci√≥n completa para la competici√≥n de Kaggle "Natural Language Processing with Disaster Tweets" utilizando t√©cnicas avanzadas de procesamiento de lenguaje natural (NLP) y fine-tuning del modelo transformer DistilBERT. El objetivo es clasificar tweets como relacionados o no relacionados con desastres naturales reales mediante an√°lisis de texto.

## üß† Descripci√≥n del Proyecto

El proyecto utiliza el modelo **DistilBERT** pre-entrenado para realizar fine-tuning espec√≠fico en el dominio de clasificaci√≥n de texto sobre desastres naturales. A trav√©s de t√©cnicas de limpieza de texto, tokenizaci√≥n avanzada y entrenamiento con datos etiquetados, se construye un clasificador binario capaz de identificar autom√°ticamente tweets que reportan desastres reales versus contenido no relacionado.

## üìä Tecnolog√≠as Utilizadas

| Categor√≠a | Tecnolog√≠a | Versi√≥n | Prop√≥sito |
|-----------|------------|---------|-----------|
| **Lenguaje** | Python | 3.x | Lenguaje principal de desarrollo |
| **Deep Learning** | PyTorch | - | Framework de deep learning |
| **Transformers** | Hugging Face Transformers | - | Modelos de lenguaje pre-entrenados |
| **Modelo Base** | DistilBERT Base Uncased | - | Modelo transformer para fine-tuning |
| **An√°lisis de Datos** | Pandas | - | Manipulaci√≥n de datasets de texto |
| **An√°lisis de Datos** | NumPy | - | Operaciones num√©ricas y vectoriales |
| **Machine Learning** | Scikit-learn | - | M√©tricas de evaluaci√≥n y divisi√≥n de datos |
| **NLP Training** | Trainer API | - | Entrenamiento simplificado de modelos |
| **M√©tricas** | F1 Score | - | Evaluaci√≥n de clasificaci√≥n binaria |
| **Preprocessing** | Regex | - | Limpieza y normalizaci√≥n de texto |

## üìÑ Pipeline de Desarrollo

### 1. **Carga y Exploraci√≥n de Datos**
```python
# Carga de datasets desde Kaggle
train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')
test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')
```

**Datasets principales:**
- **train.csv:** Tweets etiquetados como desastre (1) o no desastre (0)
- **test.csv:** Conjunto de evaluaci√≥n sin etiquetas
- **sample_submission.csv:** Formato de env√≠o requerido

### 2. **Preprocesamiento y Limpieza de Datos**

#### Funci√≥n de Limpieza de Texto
```python
def clean_text(text):
    text = text.lower()  # Convertir a min√∫sculas
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)  # Eliminar URLs
    text = re.sub(r'\@\w+|\#','', text)  # Eliminar hashtags y menciones
    text = re.sub(r"[^a-zA-Z0-9\s]", '', text)  # Eliminar caracteres especiales
    return text
```

**Beneficios del preprocesamiento:**
- **Normalizaci√≥n:** Conversi√≥n a min√∫sculas para consistencia
- **Eliminaci√≥n de ruido:** Remoci√≥n de URLs, hashtags y menciones
- **Simplificaci√≥n:** Conservaci√≥n solo de caracteres alfanum√©ricos
- **Mejora de rendimiento:** Reducci√≥n de vocabulario y ruido

#### Divisi√≥n de Datos
```python
# Divisi√≥n estratificada para training/validation
X_train, X_val, y_train, y_val = train_test_split(
    train_texts, train_labels, 
    test_size=0.2, 
    random_state=2
)
```

### 3. **Configuraci√≥n del Modelo DistilBERT**

#### Carga del Modelo Base
```python
# Carga del tokenizador y modelo DistilBERT
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", 
    num_labels=2
)
```

**Justificaci√≥n de DistilBERT:**
- **Eficiencia:** 40% m√°s peque√±o que BERT con 97% del rendimiento
- **Velocidad:** 60% m√°s r√°pido en inferencia
- **Capacidad:** Mantiene arquitectura transformer completa
- **Pre-entrenamiento:** Entrenado en corpus masivo de texto en ingl√©s

#### Tokenizaci√≥n Avanzada
```python
# Tokenizaci√≥n con par√°metros optimizados
train_encodings = tokenizer(
    X_train, 
    truncation=True, 
    padding=True, 
    max_length=256
)
val_encodings = tokenizer(
    X_val, 
    truncation=True, 
    padding=True, 
    max_length=256
)
```

**Configuraci√≥n optimizada:**
- **max_length=256:** Balance entre contexto y memoria
- **truncation=True:** Manejo de tweets largos
- **padding=True:** Uniformidad en batch processing

### 4. **Dataset Personalizado y Carga de Datos**

#### Clase Dataset Custom
```python
class NewsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
        
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item
    
    def __len__(self):
        return len(self.labels)
```

**Caracter√≠sticas del dataset:**
- **Compatibilidad PyTorch:** Integraci√≥n nativa con DataLoader
- **Eficiencia de memoria:** Carga lazy de datos
- **Flexibilidad:** Manejo de diferentes tipos de encoding

### 5. **Configuraci√≥n de Entrenamiento y Fine-Tuning**

#### Par√°metros de Entrenamiento Optimizados
```python
training_args = TrainingArguments(
    output_dir='./results',
    report_to='none',  # Desactiva logging externo
    num_train_epochs=5,  # √âpocas suficientes para convergencia
    per_device_train_batch_size=16,  # Optimizado para GPU
    per_device_eval_batch_size=32,  # Batch mayor para evaluaci√≥n
    warmup_steps=500,  # Calentamiento gradual del learning rate
    weight_decay=0.01,  # Regularizaci√≥n L2
    logging_dir='./logs',
    evaluation_strategy='steps',  # Evaluaci√≥n continua
    eval_steps=500,  # Frecuencia de evaluaci√≥n
    learning_rate=3e-5,  # Learning rate ajustado para fine-tuning
)
```

#### T√©cnica de Freezing de Capas
```python
# Congelar capas base para fine-tuning selectivo
for param in model.distilbert.parameters():
    param.requires_grad = False
```

**Ventajas del layer freezing:**
- **Prevenci√≥n de overfitting:** Mantiene representaciones pre-entrenadas
- **Velocidad de entrenamiento:** Reduce par√°metros a actualizar
- **Estabilidad:** Evita catastrophic forgetting
- **Eficiencia computacional:** Menor uso de memoria GPU

### 6. **Entrenamiento y Evaluaci√≥n del Modelo**

#### Proceso de Entrenamiento
```python
# Creaci√≥n del entrenador con configuraci√≥n optimizada
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Ejecutar fine-tuning
trainer.train()
```

#### Evaluaci√≥n y M√©tricas
```python
# Evaluaci√≥n del modelo en conjunto de validaci√≥n
trainer.evaluate()

# Predicciones y c√°lculo de F1 Score
val_predictions = trainer.predict(val_dataset)
val_pred_labels = val_predictions.predictions.argmax(-1)

# M√©trica principal: F1 Score
f1 = f1_score(y_val, val_pred_labels)
print(f'F1 Score: {f1}')
```

### 7. **Predicci√≥n en Datos de Prueba**

#### Preparaci√≥n de Datos de Test
```python
# Aplicar misma limpieza a datos de prueba
test_texts = [clean_text(text) for text in test_data['text'].tolist()]
test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=256)

# Crear dataset de prueba con labels dummy
test_dataset = NewsDataset(test_encodings, [0] * len(test_encodings['input_ids']))
```

#### Generaci√≥n de Predicciones Finales
```python
# Predicci√≥n sobre conjunto de prueba
predictions = trainer.predict(test_dataset)
predicted_labels = predictions.predictions.argmax(-1)

# Preparaci√≥n del archivo de submission
submission_df = pd.DataFrame({
    'id': test_data['id'],
    'target': predicted_labels
})

# Guardado del archivo de env√≠o
submission_df.to_csv('submission.csv', index=False)
```

## üèóÔ∏è Estructura del Proyecto (Kaggle Environment)

### Entorno de Kaggle - Archivos y Datasets Disponibles:

```
COMPETITIONS:
‚îî‚îÄ‚îÄ nlp-getting-started/
    ‚îú‚îÄ‚îÄ train.csv                              # Dataset de entrenamiento
    ‚îú‚îÄ‚îÄ test.csv                              # Dataset de evaluaci√≥n  
    ‚îî‚îÄ‚îÄ sample_submission.csv                 # Formato de env√≠o

NOTEBOOK:
‚îî‚îÄ‚îÄ disaster_classification_distilbert.ipynb  # Notebook principal del proyecto

OUTPUT (/kaggle/working/):
‚îú‚îÄ‚îÄ results/                                  # Checkpoints del modelo
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-500/                      # Checkpoint intermedio
‚îÇ   ‚îî‚îÄ‚îÄ checkpoint-1000/                     # Checkpoint final
‚îú‚îÄ‚îÄ logs/                                    # Logs de entrenamiento
‚îî‚îÄ‚îÄ submission.csv                           # Archivo de env√≠o
```

### Rutas de Acceso en C√≥digo:
```python
# Datos de entrada desde competitions
TRAIN_PATH = "/kaggle/input/nlp-getting-started/train.csv"
TEST_PATH = "/kaggle/input/nlp-getting-started/test.csv"
SAMPLE_SUBMISSION_PATH = "/kaggle/input/nlp-getting-started/sample_submission.csv"

# Outputs en working directory
RESULTS_PATH = "/kaggle/working/results"
LOGS_PATH = "/kaggle/working/logs"
SUBMISSION_PATH = "/kaggle/working/submission.csv"
```

## üöÄ C√≥mo Ejecutar el Proyecto en Kaggle

### Configuraci√≥n del Entorno Kaggle
```python
# Instalaci√≥n de librer√≠as necesarias
!pip install transformers torch

# Librer√≠as principales disponibles en Kaggle
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
import re
```

### Verificaci√≥n de GPU y Datasets
```python
# Verificar disponibilidad de GPU
print("CUDA disponible:", torch.cuda.is_available())
print("GPU:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No disponible")

# Listar archivos disponibles en input
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
```

### Ejecuci√≥n Paso a Paso

#### Paso 1: Crear y ejecutar un nuevo notebook
1. **Crear notebook** en competici√≥n "Natural Language Processing with Disaster Tweets"
2. **Configurar GPU**: Settings ‚Üí Accelerator ‚Üí GPU T4 x2
3. **Agregar datasets**:
   - **Competition Data**: Natural Language Processing with Disaster Tweets

#### Paso 2: Verificar archivos disponibles
```python
# Verificar estructura de input
import os

print("=== COMPETITION DATA ===")
for file in os.listdir('/kaggle/input/nlp-getting-started/'):
    print(f"üìÑ {file}")
```

### Flujo de Ejecuci√≥n en Kaggle

#### 1. **Preparaci√≥n de Datos**
```python
# Cargar datasets desde input directory
train_data = pd.read_csv("/kaggle/input/nlp-getting-started/train.csv")
test_data = pd.read_csv("/kaggle/input/nlp-getting-started/test.csv")

# Aplicar limpieza de texto
train_texts = [clean_text(text) for text in train_data['text'].tolist()]
train_labels = train_data['target'].tolist()
```

#### 2. **Divisi√≥n y Preparaci√≥n de Datasets**
```python
# Divisi√≥n estratificada
X_train, X_val, y_train, y_val = train_test_split(
    train_texts, train_labels, 
    test_size=0.2, 
    random_state=2
)

# Tokenizaci√≥n con DistilBERT
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=256)
val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=256)
```

#### 3. **Fine-Tuning del Modelo**
```python
# Cargar modelo pre-entrenado
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", 
    num_labels=2
)

# Aplicar layer freezing para fine-tuning selectivo
for param in model.distilbert.parameters():
    param.requires_grad = False

# Configurar training arguments para guardar en working
training_args = TrainingArguments(
    output_dir='/kaggle/working/results',
    num_train_epochs=5,
    per_device_train_batch_size=16,
    evaluation_strategy='steps',
    eval_steps=500,
)

# Entrenar modelo
trainer = Trainer(
    model=model, 
    args=training_args, 
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)
trainer.train()
```

#### 4. **Evaluaci√≥n y Predicci√≥n**
```python
# Evaluaci√≥n en conjunto de validaci√≥n
trainer.evaluate()

# Predicciones en conjunto de prueba
test_dataset = NewsDataset(test_encodings, [0] * len(test_encodings['input_ids']))
predictions = trainer.predict(test_dataset)
predicted_labels = predictions.predictions.argmax(-1)

# Guardar submission en working directory
submission_df = pd.DataFrame({
    'id': test_data['id'],
    'target': predicted_labels
})
submission_df.to_csv('/kaggle/working/submission.csv', index=False)
```

## üìà Resultados y M√©tricas

### Modelo Principal: DistilBERT Fine-Tuned
**Configuraci√≥n optimizada:**
- **Arquitectura:** DistilBERT (Distilled BERT)
- **Tama√±o:** 66M par√°metros (vs 110M de BERT-base)
- **Epochs:** 5 (balance entre convergencia y overfitting)
- **Batch Size:** 16 para entrenamiento, 32 para evaluaci√≥n
- **Learning Rate:** 3e-5 (optimizado para fine-tuning)
- **Layer Freezing:** Capas base congeladas, solo fine-tuning de clasificador

### M√©tricas de Evaluaci√≥n
- **M√©trica principal:** F1 Score (balance entre precisi√≥n y recall)
- **Estrategia de evaluaci√≥n:** Evaluaci√≥n por pasos cada 500 iteraciones
- **Regularizaci√≥n:** Weight decay 0.01 para prevenir overfitting
- **Warmup:** 500 pasos de calentamiento gradual del learning rate

### T√©cnicas de Optimizaci√≥n Implementadas
1. **Preprocesamiento robusto:** Limpieza sistem√°tica de texto
2. **Layer freezing:** Fine-tuning selectivo para eficiencia
3. **Batch size diferenciado:** Optimizaci√≥n de memoria GPU
4. **Evaluaci√≥n continua:** Monitoreo de performance durante entrenamiento

## üî¨ Innovaciones T√©cnicas

### Fortalezas del Enfoque NLP
1. **Fine-tuning selectivo:** Congelamiento de capas base para estabilidad
2. **Preprocesamiento especializado:** Limpieza adaptada a tweets
3. **Tokenizaci√≥n optimizada:** Configuraci√≥n espec√≠fica para textos cortos
4. **Evaluaci√≥n robusta:** F1 Score para datasets desbalanceados

### Aspectos √önicos del Proyecto
- **Dominio espec√≠fico:** Aplicaci√≥n de NLP a detecci√≥n de desastres
- **Eficiencia computacional:** Uso de DistilBERT para rapidez
- **Preprocessing avanzado:** Manejo especializado de contenido de redes sociales
- **Transfer learning:** Aprovechamiento de conocimiento pre-entrenado

## üéØ Posibles Mejoras

### T√©cnicas Avanzadas de NLP
1. **Ensemble de modelos:** Combinaci√≥n con RoBERTa, ALBERT, o ELECTRA
2. **Data augmentation:** Generaci√≥n sint√©tica de ejemplos de entrenamiento
3. **Curriculum learning:** Entrenamiento progresivo con ejemplos de dificultad creciente
4. **Multi-task learning:** Entrenamiento conjunto en tareas relacionadas

### Ingenier√≠a de Caracter√≠sticas Especializadas
1. **An√°lisis de sentimientos:** Incorporaci√≥n de polaridad emocional
2. **Extracci√≥n de entidades:** Identificaci√≥n de ubicaciones y eventos
3. **Caracter√≠sticas ling√º√≠sticas:** Longitud, complejidad, patrones sint√°cticos
4. **Metadatos de tweets:** Informaci√≥n temporal, geogr√°fica, de usuario

### Optimizaci√≥n de Modelo
1. **Hyperparameter tuning:** B√∫squeda sistem√°tica de par√°metros √≥ptimos
2. **Learning rate scheduling:** Estrategias de decaimiento adaptativo
3. **Gradient clipping:** Control de gradientes para estabilidad
4. **Early stopping:** Parada temprana basada en m√©tricas de validaci√≥n

### T√©cnicas de Evaluaci√≥n Avanzadas
1. **Cross-validation:** Validaci√≥n cruzada k-fold para robustez
2. **An√°lisis de errores:** Estudio cualitativo de clasificaciones incorrectas
3. **M√©tricas adicionales:** Precisi√≥n, recall, AUC-ROC
4. **An√°lisis de sesgo:** Evaluaci√≥n de fairness en diferentes grupos

## üéØ Aplicaciones del Mundo Real

### Impacto en Gesti√≥n de Emergencias
- **Detecci√≥n temprana:** Identificaci√≥n r√°pida de eventos de desastre
- **Monitoreo de redes sociales:** An√°lisis automatizado de contenido
- **Alertas autom√°ticas:** Sistemas de notificaci√≥n en tiempo real
- **An√°lisis de tendencias:** Identificaci√≥n de patrones de propagaci√≥n

### Escalabilidad y Transferencia
1. **Otros idiomas:** Adaptaci√≥n a modelos multiling√ºes
2. **Diferentes plataformas:** Extensi√≥n a Facebook, Instagram, TikTok
3. **Tipos de emergencia:** Especializaci√≥n en terremotos, incendios, inundaciones
4. **Integraci√≥n en sistemas:** APIs para centros de comando y control

## üîß Consideraciones T√©cnicas

### Consideraciones Espec√≠ficas de Kaggle

#### Limitaciones del Entorno
- **Tiempo de ejecuci√≥n:** M√°ximo 12 horas por sesi√≥n
- **Espacio en /working:** 20GB disponibles
- **GPU:** T4 x2 disponible para training
- **Internet:** Habilitado para descarga de modelos pre-entrenados
- **Librer√≠as:** Instalaci√≥n de transformers requerida

#### Optimizaciones para Kaggle
```python
# Configuraci√≥n de memoria eficiente
torch.cuda.empty_cache()

# Batch size optimizado para T4
TRAIN_BATCH_SIZE = 16  # Balance entre velocidad y memoria
EVAL_BATCH_SIZE = 32   # Batch mayor para evaluaci√≥n

# Checkpointing estrat√©gico
training_args = TrainingArguments(
    output_dir='/kaggle/working/results',
    save_steps=500,  # Guardar cada 500 pasos
    logging_steps=100,
    eval_steps=500,  # Evaluaci√≥n frecuente
)
```

#### Gesti√≥n de Archivos
```python
# Verificar espacio disponible
import shutil
total, used, free = shutil.disk_usage('/kaggle/working')
print(f"Espacio libre: {free // (2**30)} GB")

# Limpiar archivos innecesarios
import gc
gc.collect()
torch.cuda.empty_cache()
```

### Reproducibilidad
- **Random seeds:** Fijados para consistencia de resultados
- **Versiones de librer√≠as:** Especificadas para compatibilidad
- **Checkpoints:** Guardado regular durante entrenamiento
- **Logging:** Registro detallado de m√©tricas y par√°metros

### Manejo de Errores Comunes
```python
# Verificar compatibilidad de versiones
try:
    from transformers import DistilBertTokenizer
    print("‚úÖ Transformers instalado correctamente")
except ImportError:
    print("‚ùå Error: Instalar transformers con !pip install transformers")

# Verificar disponibilidad de GPU
if torch.cuda.is_available():
    print(f"‚úÖ GPU disponible: {torch.cuda.get_device_name(0)}")
else:
    print("‚ö†Ô∏è GPU no disponible, usando CPU")
```

## üìû Contacto y Colaboraci√≥n

Para consultas t√©cnicas, colaboraciones en proyectos de NLP, o discusiones sobre aplicaciones de IA en gesti√≥n de emergencias, no dudes en contactar.

## üìó Referencias y Recursos

- **DistilBERT Paper:** "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
- **BERT Architecture:** "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
- **Disaster Detection:** Literatura sobre detecci√≥n autom√°tica de desastres en redes sociales
- **Transfer Learning:** Investigaci√≥n en fine-tuning de modelos transformer

---

*Este proyecto representa una aplicaci√≥n pr√°ctica de t√©cnicas modernas de NLP para la detecci√≥n autom√°tica de desastres en redes sociales, combinando fine-tuning eficiente con preprocessing especializado para crear una herramienta √∫til en gesti√≥n de emergencias y monitoreo de eventos cr√≠ticos.*